{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "190fc799-1491-4f77-9247-252e5e696281",
   "metadata": {},
   "source": [
    "Answer 1:-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19806b64-2390-447a-b1ee-0db48a319391",
   "metadata": {},
   "source": [
    "ANOVA (Analysis of Variance) is a statistical method used to compare the means of three or more groups or treatments to determine if there are any significant differences among them.\n",
    "\n",
    "To use ANOVA and ensure the validity of its results, certain assumptions need to be met. These assumptions are :\n",
    "\n",
    "Independence : The observations within each group or treatment are independent of each other. This means that the values in one group should not be influenced or related to the values in another group.\n",
    "\n",
    "Normality : The data in each group should follow a normal distribution. The normality assumption is particularly important when the sample sizes are small, as ANOVA tends to be robust to violations when the sample sizes are large.\n",
    "\n",
    "Homogeneity of Variance (Homoscedasticity) : The variability of the data (variance) within each group should be roughly equal. In other words, the spread of the data points around the group means should be similar across all groups.\n",
    "\n",
    "Absence of outliers : There must no outliers in the data points\n",
    "\n",
    "Examples of Violations:\n",
    "\n",
    "Violation of Independence: In some experimental designs, the independence assumption may be violated if there is a hierarchical or nested structure in the data. For example, if you measure the performance of students within different classrooms, the students within the same classroom may not be independent of each other due to shared characteristics or teaching styles.\n",
    "\n",
    "Violation of Normality: If the data in any of the groups deviates significantly from a normal distribution, it can impact the validity of ANOVA results. For instance, if the data is strongly skewed or has heavy tails, the normality assumption may not hold.\n",
    "\n",
    "Violation of Homoscedasticity: Unequal variances among groups can lead to biased ANOVA results. For example, if the variability of test scores in one group is much larger than that in another group, the assumption of homogeneity of variance may not be met."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5875665-29a8-43ed-b4e0-540be6419da3",
   "metadata": {},
   "source": [
    "Answer 2:-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f027e5f-aade-4b03-951f-2d39ca85d18c",
   "metadata": {},
   "source": [
    "The three types of ANOVA (Analysis of Variance) are:\n",
    "\n",
    "One-Way ANOVA:\n",
    "\n",
    "One-Way ANOVA is used when there is one categorical independent variable (also known as a factor) with three or more levels or groups, and we want to compare the means of a continuous dependent variable across these groups.\n",
    "It is suitable for situations where we have one factor and want to determine if there are any significant differences in the means of the dependent variable across the different levels of that factor.\n",
    "\n",
    "    \n",
    "Two-Way ANOVA:\n",
    "\n",
    "Two-Way ANOVA is used when there are two categorical independent variables (factors), and we want to examine the interaction between these two factors and their effects on a continuous dependent variable.\n",
    "It is suitable for situations where we have two factors, and we want to investigate how the means of the dependent variable vary across the combinations of levels of both factors.\n",
    "\n",
    "    \n",
    "Three-Way ANOVA:\n",
    "\n",
    "Three-Way ANOVA is an extension of the two-way ANOVA and is used when there are three categorical independent variables (factors).\n",
    "It is suitable for situations where we have three factors, and we want to examine their individual effects and their interactions on a continuous dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0caa7b-3526-4f1c-b434-577aa6af20d8",
   "metadata": {},
   "source": [
    "Answer 3:-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c19c722-b69e-4ea7-93b6-0b8459e25fb7",
   "metadata": {},
   "source": [
    "Partitioning of variance in ANOVA refers to the division of the total variability in the data into different components that are associated with various sources of variation.\n",
    "\n",
    "In ANOVA, the total variance in the data is broken down into three main components:\n",
    "\n",
    "Between-Group Variance (Between-Treatments Variance): This component represents the variability in the dependent variable that can be attributed to the differences between the groups or treatments (levels of the independent variable). It measures the effect of the factors (independent variables) on the dependent variable. \n",
    "\n",
    "Within-Group Variance (Within-Treatments Variance or Residual Variance): This component represents the variability in the dependent variable that cannot be explained by the differences between the groups. It accounts for the random or unexplained variation within each group. It measures the variability of data points within each group around the group mean. \n",
    "\n",
    "Total Variance: This is the overall variability in the data, and it is the sum of the between-group variance and the within-group variance. It represents the total variation in the dependent variable across all groups and treatments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64f2d99-3349-4e07-bb27-0d8e529b0f2e",
   "metadata": {},
   "source": [
    "Answer 4:-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efadcbb7-c038-4005-9f8a-e9a19cef5371",
   "metadata": {},
   "source": [
    "Calculate the Total Sum of Squares (SST):\n",
    "SST represents the total variability in the dependent variable.\n",
    "\n",
    "The sum of squared differences between individual data points (yi) and the mean of the response variable (y).\n",
    "\n",
    "SST = Σ(yi – y)2\n",
    "\n",
    "Calculate the Explained Sum of Squares (SSE):\n",
    "SSE represents the variability in the dependent variable that can be attributed to the differences between the group means (treatments).\n",
    "The sum of squared differences between predicted data points (ŷi) and observed data points (yi).\n",
    "\n",
    "SSE = Σ(ŷi – yi)2\n",
    "\n",
    "Calculate the Residual Sum of Squares (SSR):\n",
    "SSR represents the unexplained variability in the dependent variable within each group.\n",
    "The sum of squared differences between predicted data points (ŷi) and the mean of the response variable(y).\n",
    "\n",
    "SSR = Σ(ŷi – y)2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2843d0d9-a4b8-4322-89de-80e0e26d9e83",
   "metadata": {},
   "source": [
    "Answer 5:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48db88ca-ad07-4ab8-b056-e92fb6850359",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d11557dd-7723-401d-8f67-88d22c866e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             df     sum_sq   mean_sq         F    PR(>F)\n",
      "C(Fertilizer)               1.0   0.033333  0.033333  0.012069  0.913305\n",
      "C(Watering)                 1.0   0.092308  0.092308  0.033422  0.856260\n",
      "C(Fertilizer):C(Watering)   1.0   0.057692  0.057692  0.020889  0.886118\n",
      "Residual                   28.0  77.333333  2.761905       NaN       NaN\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>height</td>      <th>  R-squared:         </th> <td>   0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>  -0.035</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td> 0.01207</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 03 Sep 2024</td> <th>  Prob (F-statistic):</th>  <td> 0.913</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>07:33:04</td>     <th>  Log-Likelihood:    </th> <td> -56.772</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    30</td>      <th>  AIC:               </th> <td>   117.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    28</td>      <th>  BIC:               </th> <td>   120.3</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                        <td></td>                           <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                                     <td>   14.8000</td> <td>    0.429</td> <td>   34.491</td> <td> 0.000</td> <td>   13.921</td> <td>   15.679</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Fertilizer)[T.weekly]</th>                       <td>   -0.0222</td> <td>    0.202</td> <td>   -0.110</td> <td> 0.913</td> <td>   -0.437</td> <td>    0.392</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Watering)[T.weekly]</th>                         <td>   -0.0222</td> <td>    0.202</td> <td>   -0.110</td> <td> 0.913</td> <td>   -0.437</td> <td>    0.392</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Fertilizer)[T.weekly]:C(Watering)[T.weekly]</th> <td>   -0.0222</td> <td>    0.202</td> <td>   -0.110</td> <td> 0.913</td> <td>   -0.437</td> <td>    0.392</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 0.177</td> <th>  Durbin-Watson:     </th> <td>   0.916</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.915</td> <th>  Jarque-Bera (JB):  </th> <td>   0.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.029</td> <th>  Prob(JB):          </th> <td>   0.995</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.929</td> <th>  Cond. No.          </th> <td>1.33e+32</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The smallest eigenvalue is 3.65e-63. This might indicate that there are<br/>strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                                &      height      & \\textbf{  R-squared:         } &     0.000   \\\\\n",
       "\\textbf{Model:}                                        &       OLS        & \\textbf{  Adj. R-squared:    } &    -0.035   \\\\\n",
       "\\textbf{Method:}                                       &  Least Squares   & \\textbf{  F-statistic:       } &   0.01207   \\\\\n",
       "\\textbf{Date:}                                         & Tue, 03 Sep 2024 & \\textbf{  Prob (F-statistic):} &    0.913    \\\\\n",
       "\\textbf{Time:}                                         &     07:33:04     & \\textbf{  Log-Likelihood:    } &   -56.772   \\\\\n",
       "\\textbf{No. Observations:}                             &          30      & \\textbf{  AIC:               } &     117.5   \\\\\n",
       "\\textbf{Df Residuals:}                                 &          28      & \\textbf{  BIC:               } &     120.3   \\\\\n",
       "\\textbf{Df Model:}                                     &           1      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}                              &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                                       & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                                     &      14.8000  &        0.429     &    34.491  &         0.000        &       13.921    &       15.679     \\\\\n",
       "\\textbf{C(Fertilizer)[T.weekly]}                       &      -0.0222  &        0.202     &    -0.110  &         0.913        &       -0.437    &        0.392     \\\\\n",
       "\\textbf{C(Watering)[T.weekly]}                         &      -0.0222  &        0.202     &    -0.110  &         0.913        &       -0.437    &        0.392     \\\\\n",
       "\\textbf{C(Fertilizer)[T.weekly]:C(Watering)[T.weekly]} &      -0.0222  &        0.202     &    -0.110  &         0.913        &       -0.437    &        0.392     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       &  0.177 & \\textbf{  Durbin-Watson:     } &    0.916  \\\\\n",
       "\\textbf{Prob(Omnibus):} &  0.915 & \\textbf{  Jarque-Bera (JB):  } &    0.011  \\\\\n",
       "\\textbf{Skew:}          &  0.029 & \\textbf{  Prob(JB):          } &    0.995  \\\\\n",
       "\\textbf{Kurtosis:}      &  2.929 & \\textbf{  Cond. No.          } & 1.33e+32  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The smallest eigenvalue is 3.65e-63. This might indicate that there are \\newline\n",
       " strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 height   R-squared:                       0.000\n",
       "Model:                            OLS   Adj. R-squared:                 -0.035\n",
       "Method:                 Least Squares   F-statistic:                   0.01207\n",
       "Date:                Tue, 03 Sep 2024   Prob (F-statistic):              0.913\n",
       "Time:                        07:33:04   Log-Likelihood:                -56.772\n",
       "No. Observations:                  30   AIC:                             117.5\n",
       "Df Residuals:                      28   BIC:                             120.3\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "=================================================================================================================\n",
       "                                                    coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------------------------------------\n",
       "Intercept                                        14.8000      0.429     34.491      0.000      13.921      15.679\n",
       "C(Fertilizer)[T.weekly]                          -0.0222      0.202     -0.110      0.913      -0.437       0.392\n",
       "C(Watering)[T.weekly]                            -0.0222      0.202     -0.110      0.913      -0.437       0.392\n",
       "C(Fertilizer)[T.weekly]:C(Watering)[T.weekly]    -0.0222      0.202     -0.110      0.913      -0.437       0.392\n",
       "==============================================================================\n",
       "Omnibus:                        0.177   Durbin-Watson:                   0.916\n",
       "Prob(Omnibus):                  0.915   Jarque-Bera (JB):                0.011\n",
       "Skew:                           0.029   Prob(JB):                        0.995\n",
       "Kurtosis:                       2.929   Cond. No.                     1.33e+32\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 3.65e-63. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "  \n",
    "# Create a dataframe\n",
    "dataframe = pd.DataFrame({'Fertilizer': np.repeat(['daily', 'weekly'], 15),\n",
    "                          'Watering': np.repeat(['daily', 'weekly'], 15),\n",
    "                          'height': [14, 16, 15, 15, 16, 13, 12, 11,\n",
    "                                     14, 15, 16, 16, 17, 18, 14, 13, \n",
    "                                     14, 14, 14, 15, 16, 16, 17, 18,\n",
    "                                     14, 13, 14, 14, 14, 15]})\n",
    "  \n",
    "  \n",
    "# Performing two-way ANOVA\n",
    "model = ols('height ~ C(Fertilizer) + C(Watering) + C(Fertilizer):C(Watering)',data=dataframe).fit()\n",
    "result = sm.stats.anova_lm(model, type=2)\n",
    "  \n",
    "# Print the result\n",
    "print(result)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1c3e8c-2543-4042-bce9-003799328c9a",
   "metadata": {},
   "source": [
    "Answer 6:-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6218c240-7e3b-40f8-80ea-08d06588f0d2",
   "metadata": {},
   "source": [
    "The p-value indicates the probability of obtaining the observed data or more extreme data under the assumption that there are no true differences between the group means (null hypothesis). A smaller p-value suggests that the observed differences are unlikely to have occurred by chance alone. Interpretation:\n",
    "\n",
    "Since the p-value (0.02) is less than the chosen significance level (α) of 0.05 (commonly used so assumption is made), we reject the null hypothesis.\n",
    "This means that there is sufficient evidence to conclude that there are statistically significant differences between the means of the groups being compared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f4cddd-a6c8-49c3-938d-55a5c15f139f",
   "metadata": {},
   "source": [
    "Answer 7:-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb9a8f8-d57e-4d6c-9f31-bb34cb68a7bb",
   "metadata": {},
   "source": [
    "There are several methods to handle missing data in a repeated measures ANOVA:\n",
    "\n",
    "Complete Case Analysis (Listwise Deletion): This method involves excluding any case with missing data in any of the variables being analyzed. While it is the simplest approach, it can lead to biased results if the data is not missing completely at random. It can also reduce the sample size and statistical power, potentially leading to less reliable results.\n",
    "\n",
    "Mean Imputation: In this method, missing data in a variable are replaced by the mean of that variable from the observed cases. While this is a straightforward approach, it may distort the distribution of the variable and underestimate the standard error, leading to overly optimistic statistical significance.\n",
    "\n",
    "Last Observation Carried Forward (LOCF): LOCF imputes missing data with the value of the last observed data point. This method assumes that the data follows a linear pattern, which may not be appropriate for all situations.\n",
    "\n",
    "Multiple Imputation: Multiple imputation involves creating multiple plausible imputations for the missing data, incorporating uncertainty in the imputation process. This approach can provide more reliable estimates and standard errors. However, it can be computationally intensive and may require making assumptions about the missing data mechanism.\n",
    "\n",
    "Maximum Likelihood Estimation (MLE): MLE is a statistical approach that estimates parameters by maximizing the likelihood function. In the context of missing data, it allows for the use of all available data and provides unbiased estimates under the assumption that data is missing at random.\n",
    "\n",
    "Pattern-Mixture Models: These models involve considering different patterns of missingness and fitting separate models for each pattern. This approach can be complex but may provide more accurate estimates when the missing data mechanism is related to the outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bd646b-3390-47fd-9208-b1d1256d9956",
   "metadata": {},
   "source": [
    "Answer 8:-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4345b5ae-1358-452d-b332-3588879272eb",
   "metadata": {},
   "source": [
    "Some common post-hoc tests include are :\n",
    "\n",
    "Tukey's Honestly Significant Difference (HSD): Tukey's HSD test is widely used when comparing all possible pairs of group means. It controls the familywise error rate, ensuring that the overall experimentwise error rate remains at a desired level (typically 0.05). This test is appropriate when you have equal group sizes and homogeneity of variances.\n",
    "\n",
    "Bonferroni Correction: The Bonferroni correction is a simple method to adjust the significance level for multiple comparisons. It divides the desired alpha level (usually 0.05) by the number of comparisons being made. This method is more conservative but can be applied to any set of comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9290f0b-763a-4349-b11d-472f24911216",
   "metadata": {},
   "source": [
    "Answer 9:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a0701d7-3e4a-48fb-a632-84d3b4e9a1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-statistic: 41.80444706032352\n",
      "p-value: 4.2309140010930765e-15\n",
      "We reject the null hypothesis.\n",
      "Final Conclusion : The mean weight loss is different for at least one diet.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Generate simulated data assuming normal distribution with same variance\n",
    "np.random.seed(20)\n",
    "diet_A = np.random.normal(5, 1, 50)\n",
    "diet_B = np.random.normal(4, 1, 50)\n",
    "diet_C = np.random.normal(3, 1, 50)\n",
    "\n",
    "# Perform one-way ANOVA\n",
    "f_statistic, p_value = stats.f_oneway(diet_A, diet_B, diet_C)\n",
    "\n",
    "# Set significance level\n",
    "alpha = 0.05\n",
    "\n",
    "# Null hypothesis: The mean weight loss is the same for all three diets.\n",
    "# Alternative hypothesis: The mean weight loss is different for at least one diet.\n",
    "null_hypothesis = \"The mean weight loss is the same for all three diets.\"\n",
    "alternate_hypothesis = \"The mean weight loss is different for at least one diet.\"\n",
    "\n",
    "print(\"F-statistic:\", f_statistic)\n",
    "print(\"p-value:\", p_value)\n",
    "if p_value < alpha:\n",
    "    print(\"We reject the null hypothesis.\")\n",
    "    print(f\"Final Conclusion : {alternate_hypothesis}\")\n",
    "else:\n",
    "    print(\"We fail to reject the null hypothesis.\")\n",
    "    print(f\"Final Conclusion : {null_hypothesis}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f3b933-0a4c-4082-81f4-9c170ebaaf7d",
   "metadata": {},
   "source": [
    "Answer 10:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e13eaeb-cd2a-4d6a-b8dd-4265ae229166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  df       sum_sq    mean_sq         F  \\\n",
      "C(Software)                      2.0     9.309580   4.654790  0.216246   \n",
      "C(ExperienceLevel)               1.0    31.851905  31.851905  1.479736   \n",
      "C(Software):C(ExperienceLevel)   2.0    52.479686  26.239843  1.219018   \n",
      "Residual                        84.0  1808.132913  21.525392       NaN   \n",
      "\n",
      "                                  PR(>F)  \n",
      "C(Software)                     0.805984  \n",
      "C(ExperienceLevel)              0.227223  \n",
      "C(Software):C(ExperienceLevel)  0.300694  \n",
      "Residual                             NaN  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Generate random data for the example (replace this with your actual data)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Software programs: A, B, C\n",
    "software_programs = np.random.choice(['A', 'B', 'C'], size=90)\n",
    "\n",
    "# Employee experience level: Novice, Experienced\n",
    "experience_level = np.random.choice(['Novice', 'Experienced'], size=90)\n",
    "\n",
    "# Random time data for each combination of program and experience level\n",
    "time_to_complete_task = np.random.normal(loc=20, scale=5, size=90)\n",
    "\n",
    "# Create a DataFrame\n",
    "data = pd.DataFrame({'Software': software_programs,\n",
    "                     'ExperienceLevel': experience_level,\n",
    "                     'Time': time_to_complete_task})\n",
    "\n",
    "# Perform the two-way ANOVA\n",
    "model = ols('Time ~ C(Software) + C(ExperienceLevel) + C(Software):C(ExperienceLevel)', data=data).fit()\n",
    "anova_table = sm.stats.anova_lm(model)\n",
    "\n",
    "# Report the results\n",
    "print(anova_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c18adc-6203-44bc-8e06-f93ff127005a",
   "metadata": {},
   "source": [
    "Answer 11:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79cb0b91-37a3-4e92-a280-85558f6aaf9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two-sample t-test:\n",
      "t-statistic: -7.738786904885968\n",
      "p-value: 5.026085102727666e-13\n",
      "There is a significant difference in test scores between the control and experimental groups.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Generate random data for the example (replace this with your actual data)\n",
    "np.random.seed(42)\n",
    "\n",
    "control_group = np.random.normal(loc=75, scale=5, size=100)\n",
    "experimental_group = np.random.normal(loc=80, scale=6, size=100)\n",
    "\n",
    "# Perform two-sample t-test\n",
    "t_statistic, p_value = stats.ttest_ind(control_group, experimental_group)\n",
    "\n",
    "# Report the results\n",
    "print(\"Two-sample t-test:\")\n",
    "print(\"t-statistic:\", t_statistic)\n",
    "print(\"p-value:\", p_value)\n",
    "\n",
    "# Interpret the results\n",
    "alpha = 0.05  # Significance level\n",
    "\n",
    "if p_value < alpha:\n",
    "    print(\"There is a significant difference in test scores between the control and experimental groups.\")\n",
    "else:\n",
    "    print(\"There is no significant difference in test scores between the control and experimental groups.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fad0f3-68a5-411c-9d54-b2c9d7927f63",
   "metadata": {},
   "source": [
    "Answer 12:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69949799-47a9-4b36-83f6-ebc3a5bb5e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tukey's HSD post-hoc test:\n",
      "  Multiple Comparison of Means - Tukey HSD, FWER=0.05   \n",
      "========================================================\n",
      " group1    group2    meandiff p-adj lower  upper  reject\n",
      "--------------------------------------------------------\n",
      "Control Experimental   5.6531   0.0 4.2125 7.0936   True\n",
      "--------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# Combine the test scores and group information into a DataFrame\n",
    "data = pd.DataFrame({'Test_Score': np.concatenate([control_group, experimental_group]),\n",
    "                     'Group': ['Control'] * 100 + ['Experimental'] * 100})\n",
    "\n",
    "# Perform Tukey's HSD post-hoc test\n",
    "tukey_results = pairwise_tukeyhsd(data['Test_Score'], data['Group'])\n",
    "\n",
    "# Report the results\n",
    "print(\"\\nTukey's HSD post-hoc test:\")\n",
    "print(tukey_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7572ba-d20b-403b-94df-43ce9ca446a7",
   "metadata": {},
   "source": [
    "Answer 12:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0a2b733-c960-4f8e-84ab-3a49bc79fb8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-way repeated measures ANOVA:\n",
      "F-statistic: 23.62763182315457\n",
      "p-value: 6.36905489476218e-09\n",
      "There is a significant difference in average daily sales between the three stores.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Generate random data for the example (replace this with your actual data)\n",
    "np.random.seed(42)\n",
    "\n",
    "store_A_sales = np.random.normal(loc=1000, scale=100, size=30)\n",
    "store_B_sales = np.random.normal(loc=950, scale=90, size=30)\n",
    "store_C_sales = np.random.normal(loc=1100, scale=110, size=30)\n",
    "\n",
    "# Combine the sales data and group information into a DataFrame\n",
    "data = pd.DataFrame({'Sales': np.concatenate([store_A_sales, store_B_sales, store_C_sales]),\n",
    "                     'Store': ['A'] * 30 + ['B'] * 30 + ['C'] * 30})\n",
    "\n",
    "# Perform one-way repeated measures ANOVA\n",
    "F_statistic, p_value = stats.f_oneway(store_A_sales, store_B_sales, store_C_sales)\n",
    "\n",
    "# Report the results\n",
    "print(\"One-way repeated measures ANOVA:\")\n",
    "print(\"F-statistic:\", F_statistic)\n",
    "print(\"p-value:\", p_value)\n",
    "\n",
    "# Interpret the results\n",
    "alpha = 0.05  # Significance level\n",
    "\n",
    "if p_value < alpha:\n",
    "    print(\"There is a significant difference in average daily sales between the three stores.\")\n",
    "else:\n",
    "    print(\"There is no significant difference in average daily sales between the three stores.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90830ddf-327b-43df-b0bf-6bb6c369869c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
